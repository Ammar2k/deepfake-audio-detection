{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AqeEF1o-oh8j",
        "outputId": "4666b2de-4a0b-4f74-f74d-16c615af7941"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets[audio] in /usr/local/lib/python3.10/dist-packages (2.16.1)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.2.0)\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.10/dist-packages (0.10.1)\n",
            "Requirement already satisfied: pyannote.audio in /usr/local/lib/python3.10/dist-packages (3.1.1)\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (1.6.4)\n",
            "Requirement already satisfied: onnxruntime in /usr/local/lib/python3.10/dist-packages (1.17.0)\n",
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.10/dist-packages (0.4.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets[audio]) (3.13.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets[audio]) (1.23.5)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets[audio]) (10.0.1)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets[audio]) (0.6)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets[audio]) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets[audio]) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets[audio]) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets[audio]) (4.66.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets[audio]) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets[audio]) (0.70.15)\n",
            "Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets[audio]) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets[audio]) (3.9.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.10/dist-packages (from datasets[audio]) (0.20.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets[audio]) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets[audio]) (6.0.1)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.10/dist-packages (from datasets[audio]) (0.12.1)\n",
            "Requirement already satisfied: torch==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torchaudio) (2.2.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0->torchaudio) (4.9.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0->torchaudio) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0->torchaudio) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0->torchaudio) (3.1.3)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0->torchaudio) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0->torchaudio) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0->torchaudio) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0->torchaudio) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0->torchaudio) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0->torchaudio) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0->torchaudio) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0->torchaudio) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0->torchaudio) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0->torchaudio) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0->torchaudio) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0->torchaudio) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2.0->torchaudio) (12.3.101)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.10/dist-packages (from librosa) (3.0.1)\n",
            "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.11.4)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.2.2)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.3.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.58.1)\n",
            "Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.8.0)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.3.7)\n",
            "Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.3)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.0.7)\n",
            "Requirement already satisfied: asteroid-filterbanks>=0.4 in /usr/local/lib/python3.10/dist-packages (from pyannote.audio) (0.4.0)\n",
            "Requirement already satisfied: einops>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pyannote.audio) (0.7.0)\n",
            "Requirement already satisfied: lightning>=2.0.1 in /usr/local/lib/python3.10/dist-packages (from pyannote.audio) (2.1.3)\n",
            "Requirement already satisfied: omegaconf<3.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from pyannote.audio) (2.3.0)\n",
            "Requirement already satisfied: pyannote.core>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from pyannote.audio) (5.0.0)\n",
            "Requirement already satisfied: pyannote.database>=5.0.1 in /usr/local/lib/python3.10/dist-packages (from pyannote.audio) (5.0.1)\n",
            "Requirement already satisfied: pyannote.metrics>=3.2 in /usr/local/lib/python3.10/dist-packages (from pyannote.audio) (3.2.1)\n",
            "Requirement already satisfied: pyannote.pipeline>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from pyannote.audio) (3.0.1)\n",
            "Requirement already satisfied: pytorch-metric-learning>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from pyannote.audio) (2.4.1)\n",
            "Requirement already satisfied: rich>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from pyannote.audio) (13.7.0)\n",
            "Requirement already satisfied: semver>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from pyannote.audio) (3.0.2)\n",
            "Requirement already satisfied: speechbrain>=0.5.14 in /usr/local/lib/python3.10/dist-packages (from pyannote.audio) (0.5.16)\n",
            "Requirement already satisfied: tensorboardX>=2.6 in /usr/local/lib/python3.10/dist-packages (from pyannote.audio) (2.6.2.2)\n",
            "Requirement already satisfied: torch-audiomentations>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from pyannote.audio) (0.11.0)\n",
            "Requirement already satisfied: torchmetrics>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from pyannote.audio) (1.3.0.post0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle) (1.16.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from kaggle) (2023.11.17)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle) (8.0.2)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.0.7)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle) (6.1.0)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (23.5.26)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (3.20.3)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.18.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets[audio]) (23.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets[audio]) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets[audio]) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets[audio]) (1.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets[audio]) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets[audio]) (4.0.3)\n",
            "Requirement already satisfied: lightning-utilities<2.0,>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from lightning>=2.0.1->pyannote.audio) (0.10.1)\n",
            "Requirement already satisfied: pytorch-lightning in /usr/local/lib/python3.10/dist-packages (from lightning>=2.0.1->pyannote.audio) (2.1.3)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.0->librosa) (0.41.1)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.10/dist-packages (from omegaconf<3.0,>=2.1->pyannote.audio) (4.9.3)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.0->librosa) (4.1.0)\n",
            "Requirement already satisfied: sortedcontainers>=2.0.4 in /usr/local/lib/python3.10/dist-packages (from pyannote.core>=5.0.0->pyannote.audio) (2.4.0)\n",
            "Requirement already satisfied: typer[all]>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from pyannote.database>=5.0.1->pyannote.audio) (0.9.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets[audio]) (2023.3.post1)\n",
            "Requirement already satisfied: docopt>=0.6.2 in /usr/local/lib/python3.10/dist-packages (from pyannote.metrics>=3.2->pyannote.audio) (0.6.2)\n",
            "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from pyannote.metrics>=3.2->pyannote.audio) (0.9.0)\n",
            "Requirement already satisfied: matplotlib>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pyannote.metrics>=3.2->pyannote.audio) (3.7.1)\n",
            "Requirement already satisfied: optuna>=3.1 in /usr/local/lib/python3.10/dist-packages (from pyannote.pipeline>=3.0.1->pyannote.audio) (3.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets[audio]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets[audio]) (3.6)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=12.0.0->pyannote.audio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=12.0.0->pyannote.audio) (2.16.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->librosa) (3.2.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile>=0.12.1->datasets[audio]) (1.16.0)\n",
            "Requirement already satisfied: hyperpyyaml in /usr/local/lib/python3.10/dist-packages (from speechbrain>=0.5.14->pyannote.audio) (1.2.2)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from speechbrain>=0.5.14->pyannote.audio) (0.1.99)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.2.0->torchaudio) (1.3.0)\n",
            "Requirement already satisfied: julius<0.3,>=0.2.3 in /usr/local/lib/python3.10/dist-packages (from torch-audiomentations>=0.11.0->pyannote.audio) (0.2.7)\n",
            "Requirement already satisfied: torch-pitch-shift>=1.2.2 in /usr/local/lib/python3.10/dist-packages (from torch-audiomentations>=0.11.0->pyannote.audio) (1.2.4)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle) (0.5.1)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.10/dist-packages (from coloredlogs->onnxruntime) (10.0)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile>=0.12.1->datasets[audio]) (2.21)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities<2.0,>=0.8.0->lightning>=2.0.1->pyannote.audio) (67.7.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=12.0.0->pyannote.audio) (0.1.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio) (4.47.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio) (1.4.5)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio) (3.1.1)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from optuna>=3.1->pyannote.pipeline>=3.0.1->pyannote.audio) (1.13.1)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.10/dist-packages (from optuna>=3.1->pyannote.pipeline>=3.0.1->pyannote.audio) (6.8.2)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from optuna>=3.1->pyannote.pipeline>=3.0.1->pyannote.audio) (2.0.24)\n",
            "Requirement already satisfied: primePy>=1.3 in /usr/local/lib/python3.10/dist-packages (from torch-pitch-shift>=1.2.2->torch-audiomentations>=0.11.0->pyannote.audio) (1.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer[all]>=0.2.1->pyannote.database>=5.0.1->pyannote.audio) (8.1.7)\n",
            "Requirement already satisfied: colorama<0.5.0,>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from typer[all]>=0.2.1->pyannote.database>=5.0.1->pyannote.audio) (0.4.6)\n",
            "Requirement already satisfied: shellingham<2.0.0,>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer[all]>=0.2.1->pyannote.database>=5.0.1->pyannote.audio) (1.5.4)\n",
            "Requirement already satisfied: ruamel.yaml>=0.17.28 in /usr/local/lib/python3.10/dist-packages (from hyperpyyaml->speechbrain>=0.5.14->pyannote.audio) (0.18.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.2.0->torchaudio) (2.1.4)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna>=3.1->pyannote.pipeline>=3.0.1->pyannote.audio) (1.3.2)\n",
            "Requirement already satisfied: ruamel.yaml.clib>=0.2.7 in /usr/local/lib/python3.10/dist-packages (from ruamel.yaml>=0.17.28->hyperpyyaml->speechbrain>=0.5.14->pyannote.audio) (0.2.8)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.3.0->optuna>=3.1->pyannote.pipeline>=3.0.1->pyannote.audio) (3.0.3)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.26.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.2.0)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.20.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.9.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.3.101)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.37.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.11.17)\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets[audio] torchaudio librosa pyannote.audio kaggle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2ZWomtBEuG_"
      },
      "source": [
        "## Loading dataset from kaggle\n",
        "\n",
        "Ensure that you have [kaggle key](https://www.kaggle.com/docs/api) in your directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "xIOadgcR3nVx"
      },
      "outputs": [],
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!cp /content/kaggle.json ~/.kaggle/ # path to your kaggle key\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tikO1cZP3sEA",
        "outputId": "74824f77-6400-45a6-8227-5c770d9b8a0f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading deep-voice-deepfake-voice-recognition.zip to /content\n",
            "100% 3.68G/3.69G [02:54<00:00, 24.8MB/s]\n",
            "100% 3.69G/3.69G [02:54<00:00, 22.7MB/s]\n"
          ]
        }
      ],
      "source": [
        "!kaggle datasets download -d birdy654/deep-voice-deepfake-voice-recognition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "k3IPwp2A5U84"
      },
      "outputs": [],
      "source": [
        "!unzip -q /content/deep-voice-deepfake-voice-recognition.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchaudio\n",
        "from torchaudio import load\n",
        "from pathlib import Path\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from sklearn.model_selection import train_test_split\n",
        "from datasets import load_dataset, Audio, concatenate_datasets, Dataset\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "from glob import glob\n",
        "from random import random\n",
        "import librosa\n",
        "import torchaudio\n",
        "import warnings\n",
        "from pyannote.audio import Model\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "xcjaYhFZwWUm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32e729e6-9b8d-4df4-e391-406d606ccdb8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/usr/local/lib/python3.10/dist-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/pyannote/audio/core/io.py:43: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.\n",
            "  torchaudio.set_audio_backend(\"soundfile\")\n",
            "/usr/local/lib/python3.10/dist-packages/torch_audiomentations/utils/io.py:27: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.\n",
            "  torchaudio.set_audio_backend(\"soundfile\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cl_jPB0aMh6H"
      },
      "source": [
        "## Augmenting real data\n",
        "Since we have less files with human audio, we generate augmented samples of those so we have more of a balanced dataset for fine-tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "RYs3hjBOMeP4"
      },
      "outputs": [],
      "source": [
        "import librosa\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "import soundfile as sf\n",
        "\n",
        "# Function to load audio file using Librosa\n",
        "def load_audio(file_path, target_sr=16000):\n",
        "    audio, _ = librosa.load(file_path, sr=target_sr)\n",
        "    return audio\n",
        "\n",
        "# Function to add random noise to audio\n",
        "def add_noise(audio, noise_level=0.005):\n",
        "    noise = np.random.normal(0, noise_level, len(audio))\n",
        "    augmented_audio = audio + noise\n",
        "    return augmented_audio\n",
        "\n",
        "# Function to perform time stretching on audio\n",
        "def time_stretch(audio, rate=1.2):\n",
        "    augmented_audio = librosa.effects.time_stretch(audio, rate=rate)\n",
        "    return augmented_audio\n",
        "\n",
        "# Function to perform pitch shifting on audio\n",
        "def pitch_shift(audio, semitone_steps=2):\n",
        "    augmented_audio = librosa.effects.pitch_shift(audio, sr=16000, n_steps=semitone_steps)\n",
        "    return augmented_audio\n",
        "\n",
        "# Function to save augmented audio\n",
        "def save_audio(audio, output_path, sr=16000):\n",
        "  \"\"\"Saves augmented audio using soundfile.\"\"\"\n",
        "  sf.write(output_path, audio, sr, subtype='PCM_16')\n",
        "\n",
        "\n",
        "# Function to augment audio and save the augmented samples\n",
        "def augment_and_save(input_folder, output_folder, num_augmentations=5):\n",
        "    # Ensure output folder exists\n",
        "    if not os.path.exists(output_folder):\n",
        "        os.makedirs(output_folder)\n",
        "\n",
        "    # Iterate through audio files in the input folder\n",
        "    for filename in os.listdir(input_folder):\n",
        "        if filename.endswith(\".wav\"):\n",
        "            file_path = os.path.join(input_folder, filename)\n",
        "            audio = load_audio(file_path)\n",
        "\n",
        "            # Augment and save multiple times\n",
        "            for i in range(num_augmentations):\n",
        "                augmented_audio = audio\n",
        "\n",
        "                # Apply random augmentation\n",
        "                augmentation_type = random.choice(['noise', 'time_stretch', 'pitch_shift'])\n",
        "                if augmentation_type == 'noise':\n",
        "                    augmented_audio = add_noise(augmented_audio)\n",
        "                elif augmentation_type == 'time_stretch':\n",
        "                    augmented_audio = time_stretch(augmented_audio)\n",
        "                elif augmentation_type == 'pitch_shift':\n",
        "                    augmented_audio = pitch_shift(augmented_audio)\n",
        "\n",
        "                # Save augmented audio\n",
        "                output_filename = f\"{os.path.splitext(filename)[0]}_aug_{i+1}.wav\"\n",
        "                output_path = os.path.join(output_folder, output_filename)\n",
        "                save_audio(augmented_audio, output_path)\n",
        "\n",
        "# Example usage\n",
        "input_folder = \"/content/KAGGLE/AUDIO/REAL\"\n",
        "output_folder = \"/content/KAGGLE/AUDIO/REAL\"\n",
        "augment_and_save(input_folder, output_folder, num_augmentations=3)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ba76G3xCo1ir"
      },
      "source": [
        "## Loading audio files\n",
        "- This following code reads .wav files from a directory, converts them to labeled audio features, and resamples them to a uniform format.\n",
        "- Hugging Face datasets are used to manage the data, including splitting into training, validation, and test sets.\n",
        "- The datasets are loaded into PyTorch Dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_file_list(directory):\n",
        "    \"\"\"Prints the audio length in seconds for all .wav files in a directory and its subdirectories.\"\"\"\n",
        "    # Create an empty list to store the file paths\n",
        "    path_list = []\n",
        "    # Walk through the directory and its subdirectories\n",
        "    for root, _, filenames in os.walk(directory):\n",
        "        # Iterate through the files\n",
        "        for filename in filenames:\n",
        "            # Check if the file is a .wav file\n",
        "            if filename.endswith(\".wav\"):\n",
        "                # Create the file path by joining the root and the file name\n",
        "                filepath = os.path.join(root, filename)\n",
        "                # Add the file path to the list\n",
        "                path_list.append(filepath)\n",
        "    # Return the list of .wav file paths\n",
        "    return path_list\n",
        "\n",
        "def create_dataset_from_directory(directory, label, max_files=None):\n",
        "    \"\"\"Creates a labelled dataset from a directory of .wav files.\"\"\"\n",
        "    # Get the list of .wav file paths using the get_file_list function\n",
        "    path_list = get_file_list(directory)\n",
        "    # If the max_files parameter is not None, limit the file paths to the specified number\n",
        "    if max_files is not None:\n",
        "        path_list = path_list[:max_files]\n",
        "    # Create a dataset from the list of file paths with the \"audio\" column\n",
        "    audio_dataset = Dataset.from_dict({\"audio\": path_list}).cast_column(\"audio\", Audio())\n",
        "    # Convert the dataset to a pandas dataframe\n",
        "    df = audio_dataset.to_pandas()\n",
        "    # Add a 'label' column to the dataframe with the specified label\n",
        "    df['labels'] = label\n",
        "    # Create a new dataset from the pandas dataframe\n",
        "    dataset_labelled = Dataset.from_pandas(df)\n",
        "    # Convert the \"audio\" column to the Audio feature, resampling to a sampling rate of 16000 Hz and converting to mono\n",
        "    dataset_labelled = dataset_labelled.cast_column(\"audio\", Audio(sampling_rate=16000, mono=True)).rename_column(\"audio\", \"input_values\")\n",
        "    # Return the labelled dataset\n",
        "    return dataset_labelled"
      ],
      "metadata": {
        "id": "1fHRPvREwdjW"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creating datasets from directory\n",
        "real_dataset = create_dataset_from_directory(\"/content/KAGGLE/AUDIO/REAL\", label=1)\n",
        "fake_dataset = create_dataset_from_directory(\"/content/KAGGLE/AUDIO/FAKE\", label=0)\n",
        "\n",
        "# Combine the datasets\n",
        "combined_dataset = concatenate_datasets([real_dataset, fake_dataset])"
      ],
      "metadata": {
        "id": "1gFf37RTweg3"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collate(batch):\n",
        "    # get the audio\n",
        "    audio = torch.tensor([sample['audio']['array'] for sample in batch])\n",
        "    # Get the labels\n",
        "    labels = torch.tensor([sample['label'] for sample in batch])\n",
        "    # Return the padded audio and labels\n",
        "    return {'input_values': audio, 'labels': labels}\n",
        "\n",
        "# Set a random seed for reproducibility\n",
        "random_seed = 42  # You can choose any arbitrary integer as the seed\n",
        "torch.manual_seed(random_seed)\n",
        "\n",
        "# Perform the random split for training and validation datasets\n",
        "train_size = int(0.8 * len(combined_dataset))\n",
        "val_size = len(combined_dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(combined_dataset, [train_size, val_size])\n",
        "\n",
        "# Further split the validation set into validation and test sets\n",
        "val_size = int(0.5 * len(val_dataset))\n",
        "test_size = len(val_dataset) - val_size\n",
        "val_dataset, test_dataset = random_split(val_dataset, [val_size, test_size], generator=torch.Generator().manual_seed(random_seed))\n",
        "\n",
        "# Create DataLoaders for the training, validation, and test datasets\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=1, collate_fn=collate)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=1, collate_fn=collate)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=1, collate_fn=collate)\n",
        "\n",
        "len(train_dataloader), len(val_dataloader), len(test_dataloader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5oz8RKaLwoOp",
        "outputId": "315be23a-83de-4c47-ce6b-94d9393c1d23"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(70, 9, 9)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q6RaRNOKq07v"
      },
      "source": [
        "## Download the pre-trained model\n",
        "For this example, we are using the WeSpeaker model, pre-trained on voxceleb dataset, wrapped by pyannote-audio library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "nb0lyH4dqstY"
      },
      "outputs": [],
      "source": [
        "from pyannote.audio import Model\n",
        "\n",
        "base_model = Model.from_pretrained(\"pyannote/wespeaker-voxceleb-resnet34-LM\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UoGgTeVDuKZ9",
        "outputId": "b477bfce-0ea3-4525-a74c-bff9a26255df"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "import torch\n",
        "# setting device to GPU if available, othervise CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LPmXiOcex_Da"
      },
      "source": [
        "## Preparing for fine-tuning\n",
        "Freeze all the layers of the base model.\n",
        "Add a classifier head for Binary Classification."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "li5gujVWsMOR"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "\n",
        "class WeSpeakerResNet34WithClassifier(nn.Module):\n",
        "    def __init__(self, model: nn.Module, num_classes=2):\n",
        "        super().__init__()\n",
        "\n",
        "        # Move the model to GPU if available\n",
        "        self.base_model = model.to(device)\n",
        "\n",
        "        # Freeze the parameters of the base model\n",
        "        for param in self.base_model.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        self.classifier = nn.Linear(256, 64).to(device)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.base_model(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "# Move the model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Instantiate the custom network with the base model and specify the number of classes\n",
        "model = WeSpeakerResNet34WithClassifier(num_classes=2, model=base_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NXpIJCEf0BhZ"
      },
      "source": [
        "# Run the model through a training loop"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import time\n",
        "import torch.nn as nn\n",
        "from tqdm import tqdm  # Assuming you have tqdm installed\n",
        "\n",
        "def train_model(model, train_dataloader, val_dataloader, num_epochs=5, learning_rate=0.001):\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.AdamW(model.classifier.parameters(), lr=learning_rate)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"Epoch number: {epoch + 1}\")\n",
        "\n",
        "        # Training phase\n",
        "        model.train()  # Set model to training mode\n",
        "\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        with tqdm(total=len(train_dataloader)) as pbar:\n",
        "            for i, batch in enumerate(train_dataloader):\n",
        "                # Move data to GPU if available\n",
        "                audio = batch[\"audio\"].unsqueeze(0).float().to(device)\n",
        "                label = batch['label'].long().to(device)\n",
        "\n",
        "                # Forward pass, loss calculation, optimizer step (same as before)\n",
        "                output = model(audio)\n",
        "                loss = criterion(output, label)\n",
        "                running_loss += loss.item()\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                # Update progress bar and accuracy metrics (same as before)\n",
        "                pbar.update(1)\n",
        "                _, predicted = torch.max(output.data, 1)\n",
        "                total += label.size(0)\n",
        "                correct += (predicted == label).sum().item()\n",
        "\n",
        "        # Calculate training loss and accuracy\n",
        "        average_loss = running_loss / len(train_dataloader)\n",
        "        accuracy = correct / total * 100 if total != 0 else 0\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()  # Set model to evaluation mode\n",
        "\n",
        "        val_running_loss = 0.0\n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "\n",
        "        with torch.no_grad():  # Disable gradient calculation for validation\n",
        "            for val_batch in val_dataloader:\n",
        "                val_audio = val_batch[\"audio\"].unsqueeze(0).float().to(device)\n",
        "                val_label = val_batch['label'].long().to(device)\n",
        "\n",
        "                val_output = model(val_audio)\n",
        "                val_loss = criterion(val_output, val_label)\n",
        "                val_running_loss += val_loss.item()\n",
        "\n",
        "                _, val_predicted = torch.max(val_output.data, 1)\n",
        "                val_total += val_label.size(0)\n",
        "                val_correct += (val_predicted == val_label).sum().item()\n",
        "\n",
        "        # Calculate validation loss and accuracy\n",
        "        val_average_loss = val_running_loss / len(val_dataloader)\n",
        "        val_accuracy = val_correct / val_total * 100 if val_total != 0 else 0\n",
        "\n",
        "        # Print training and validation results\n",
        "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Training Loss: {average_loss:.4f}, Training Accuracy: {accuracy:.2f}%, Validation Loss: {val_average_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%\")\n"
      ],
      "metadata": {
        "id": "55o-4LM1iQzh"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_model(model, train_dataloader, val_dataloader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lfMTsaEciTEc",
        "outputId": "2f507158-7a85-4696-8c97-582dfa3fbf13"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch number: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 70/70 [03:17<00:00,  2.83s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/5], Training Loss: nan, Training Accuracy: 67.14%, Validation Loss: nan, Validation Accuracy: 33.33%\n",
            "Epoch number: 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 70/70 [03:15<00:00,  2.80s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [2/5], Training Loss: nan, Training Accuracy: 67.14%, Validation Loss: nan, Validation Accuracy: 33.33%\n",
            "Epoch number: 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 70/70 [03:10<00:00,  2.72s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [3/5], Training Loss: nan, Training Accuracy: 67.14%, Validation Loss: nan, Validation Accuracy: 33.33%\n",
            "Epoch number: 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 70/70 [03:07<00:00,  2.67s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [4/5], Training Loss: nan, Training Accuracy: 67.14%, Validation Loss: nan, Validation Accuracy: 33.33%\n",
            "Epoch number: 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 70/70 [03:03<00:00,  2.63s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [5/5], Training Loss: nan, Training Accuracy: 67.14%, Validation Loss: nan, Validation Accuracy: 33.33%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation\n",
        "The following code is used for evaluation the model. It calculates the accuracy, precision, recall and the F1 score on the given dataloader. It also prints the confusion matrix"
      ],
      "metadata": {
        "id": "rtymc7aSw09J"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "iMbZVReQ0Nwt"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# Function to plot a confusion matrix\n",
        "def plot_confusion_matrix(y_true, y_pred, labels):\n",
        "    # Compute the confusion matrix using scikit-learn's confusion_matrix function\n",
        "    cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
        "\n",
        "    # Create a figure for the plot with specified size\n",
        "    plt.figure(figsize=(8, 6))\n",
        "\n",
        "    # Plot the confusion matrix as a heatmap with annotations\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)\n",
        "\n",
        "    # Set labels for the x and y axes\n",
        "    plt.xlabel('Predicted Labels')\n",
        "    plt.ylabel('True Labels')\n",
        "\n",
        "    # Set the title of the plot\n",
        "    plt.title('Confusion Matrix')\n",
        "\n",
        "    # Display the plot\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def process_audio_batch(model, dataloader):\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "\n",
        "    # initializing empty lists to store prediction and ground truth\n",
        "    predictions = []\n",
        "    ground_truths = []\n",
        "\n",
        "    # initializing labels\n",
        "    labels = [\"fake\", \"real\"]\n",
        "\n",
        "    # Pass each batch through the model without gradient calculation\n",
        "    for batch in dataloader:\n",
        "        # Move data to GPU if available\n",
        "        audio = batch[\"audio\"].unsqueeze(0).float().to(device)\n",
        "        label = batch['label'].long().to(device)\n",
        "        # run the model in inference mode\n",
        "        with torch.no_grad():\n",
        "            output = model(audio)\n",
        "        # add the predictions and ground truth to the list\n",
        "        predictions.extend(output.argmax(dim=1).tolist())\n",
        "        ground_truths.extend(label.tolist())\n",
        "\n",
        "    # Check if all labels are present in the ground truth\n",
        "    unique_labels = set(ground_truths + predictions)\n",
        "    if not set(labels).issubset(unique_labels):\n",
        "        # print(\"Warning: Not all specified labels are present in the ground truth.\")\n",
        "        # print(f\"Present labels in ground truth: {unique_labels}\")\n",
        "        labels = list(unique_labels)\n",
        "\n",
        "    # Calculate metrics after processing all batches\n",
        "    accuracy = accuracy_score(ground_truths, predictions)\n",
        "    precision = precision_score(ground_truths, predictions, average='weighted')\n",
        "    recall = recall_score(ground_truths, predictions, average='weighted')\n",
        "    f1 = f1_score(ground_truths, predictions, average='weighted')\n",
        "\n",
        "    # Create confusion matrix\n",
        "    plot_confusion_matrix(ground_truths, predictions, labels)\n",
        "\n",
        "    return predictions, ground_truths, accuracy, precision, recall, f1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "Yz-ZzLNiB9xy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 633
        },
        "outputId": "16844566-b2a7-42a1-dc3b-970c8509baa0"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.6666666666666666\n",
            "Precision: 0.4444444444444444\n",
            "Recall 0.6666666666666666\n",
            "F1 Score: 0.5333333333333334\n"
          ]
        }
      ],
      "source": [
        "_, _, accuracy, precision, recall, f1 = process_audio_batch(model, test_dataloader)\n",
        "print(f\"Accuracy: {accuracy}\\nPrecision: {precision}\\nRecall {recall}\\nF1 Score: {f1}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ZUmQOR9M8Jy"
      },
      "source": [
        "## Saving the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tL2jlM8gFQvU"
      },
      "outputs": [],
      "source": [
        "torch.save(model, '/content/model.pt')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}