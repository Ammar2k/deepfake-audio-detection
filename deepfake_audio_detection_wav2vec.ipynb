{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4vZr1j1J7cst"
      },
      "outputs": [],
      "source": [
        "!pip install datasets evaluate torch pydub\n",
        "!pip install -U accelerate\n",
        "!pip install -U transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Connecting to HuggingFace, this is needed to get access to pre-trained models"
      ],
      "metadata": {
        "id": "JaRVu3EL7kiL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "cTTx3cky7dcm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading dataset from kaggle\n",
        "\n",
        "Ensure that you have [kaggle key](https://www.kaggle.com/docs/api) in your directory"
      ],
      "metadata": {
        "id": "BuSdjy_f7yrP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!cp /content/kaggle.json ~/.kaggle/ # path to your kaggle key\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "!kaggle datasets download -d birdy654/deep-voice-deepfake-voice-recognition\n",
        "!unzip -q /content/deep-voice-deepfake-voice-recognition.zip"
      ],
      "metadata": {
        "id": "zgRWRln47de4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "importing required libraries"
      ],
      "metadata": {
        "id": "HZLypgVM75hI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pydub import AudioSegment\n",
        "from pydub.utils import make_chunks\n",
        "import pandas as pd\n",
        "import evaluate\n",
        "import numpy as np\n",
        "from transformers import AutoModelForAudioClassification, TrainingArguments, Trainer, Wav2Vec2Processor\n",
        "from datasets import load_dataset, Audio, Dataset, concatenate_datasets, ClassLabel, Features, Value"
      ],
      "metadata": {
        "id": "F3xGskmA7dhN"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Augmenting real data\n",
        "Since we have less files with human audio, we generate augmented samples of those so we have more of a balanced dataset for fine-tuning"
      ],
      "metadata": {
        "id": "E2dnOQlv8Ak2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to load audio file using Librosa\n",
        "def load_audio(file_path, target_sr=16000):\n",
        "    audio, _ = librosa.load(file_path, sr=target_sr)\n",
        "    return audio\n",
        "\n",
        "# Function to add random noise to audio\n",
        "def add_noise(audio, noise_level=0.005):\n",
        "    noise = np.random.normal(0, noise_level, len(audio))\n",
        "    augmented_audio = audio + noise\n",
        "    return augmented_audio\n",
        "\n",
        "# Function to perform time stretching on audio\n",
        "def time_stretch(audio, rate=1.2):\n",
        "    augmented_audio = librosa.effects.time_stretch(audio, rate=rate)\n",
        "    return augmented_audio\n",
        "\n",
        "# Function to perform pitch shifting on audio\n",
        "def pitch_shift(audio, semitone_steps=2):\n",
        "    augmented_audio = librosa.effects.pitch_shift(audio, sr=16000, n_steps=semitone_steps)\n",
        "    return augmented_audio\n",
        "\n",
        "# Function to save augmented audio\n",
        "def save_audio(audio, output_path, sr=16000):\n",
        "  \"\"\"Saves augmented audio using soundfile.\"\"\"\n",
        "  sf.write(output_path, audio, sr, subtype='PCM_16')\n",
        "\n",
        "\n",
        "# Function to augment audio and save the augmented samples\n",
        "def augment_and_save(input_folder, output_folder, num_augmentations=5):\n",
        "    # Ensure output folder exists\n",
        "    if not os.path.exists(output_folder):\n",
        "        os.makedirs(output_folder)\n",
        "\n",
        "    # Iterate through audio files in the input folder\n",
        "    for filename in os.listdir(input_folder):\n",
        "        if filename.endswith(\".wav\"):\n",
        "            file_path = os.path.join(input_folder, filename)\n",
        "            audio = load_audio(file_path)\n",
        "\n",
        "            # Augment and save multiple times\n",
        "            for i in range(num_augmentations):\n",
        "                augmented_audio = audio\n",
        "\n",
        "                # Apply random augmentation\n",
        "                augmentation_type = random.choice(['noise', 'time_stretch', 'pitch_shift'])\n",
        "                if augmentation_type == 'noise':\n",
        "                    augmented_audio = add_noise(augmented_audio)\n",
        "                elif augmentation_type == 'time_stretch':\n",
        "                    augmented_audio = time_stretch(augmented_audio)\n",
        "                elif augmentation_type == 'pitch_shift':\n",
        "                    augmented_audio = pitch_shift(augmented_audio)\n",
        "\n",
        "                # Save augmented audio\n",
        "                output_filename = f\"{os.path.splitext(filename)[0]}_aug_{i+1}.wav\"\n",
        "                output_path = os.path.join(output_folder, output_filename)\n",
        "                save_audio(augmented_audio, output_path)\n",
        "\n",
        "# Example usage\n",
        "input_folder = \"/content/KAGGLE/AUDIO/REAL\"\n",
        "output_folder = \"/content/KAGGLE/AUDIO/REAL\"\n",
        "augment_and_save(input_folder, output_folder, num_augmentations=3)"
      ],
      "metadata": {
        "id": "3wWB-Pon7dkA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Converting to 15 sec chunks\n",
        "Here, we convert the audio files to 15-sec chunks. The smaller chunks will be passed to the model for training."
      ],
      "metadata": {
        "id": "FfTd63Hc8KXv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# function to create smaller audio chunks\n",
        "def create_shorter_chunks(directory, chunk_length_ms=15000):\n",
        "    # Iterate over all files in the directory\n",
        "    for filename in os.listdir(directory):\n",
        "        # Check if the file is an audio file\n",
        "        if filename.endswith(('.mp3', '.wav', '.ogg', '.flac')):\n",
        "            # Create an AudioSegment instance\n",
        "            audio_segment = AudioSegment.from_file(os.path.join(directory, filename))\n",
        "\n",
        "            # Divide the audio into chunks\n",
        "            chunks = make_chunks(audio_segment, chunk_length_ms)\n",
        "\n",
        "            # Save each chunk with a new name\n",
        "            for i, chunk in enumerate(chunks):\n",
        "                # Create a new filename for each chunk\n",
        "                chunk_filename = f\"{os.path.splitext(filename)[0]}_chunk{i}.wav\"\n",
        "                # Save the chunk to the same directory\n",
        "                chunk.export(os.path.join(directory, chunk_filename), format=\"wav\")\n",
        "    print(f\"Audio files in {directory} have been divided into 15-second chunks.\")\n",
        "\n",
        "# Define the directory where the audio files are located\n",
        "create_shorter_chunks('/content/KAGGLE/AUDIO/REAL')\n",
        "create_shorter_chunks('/content/KAGGLE/AUDIO/FAKE')"
      ],
      "metadata": {
        "id": "lTRtA55p8SlF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating and formatting dataset\n",
        "The following code block creates a dataset from the given directory, and adds associated labels to it."
      ],
      "metadata": {
        "id": "3Bq790_O9GHp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# returns a list of all .wav and .mp3 files in a given directory\n",
        "def get_file_list(directory):\n",
        "    \"\"\"Returns a list of file paths for all .wav and .mp3 files in a directory and its subdirectories.\"\"\"\n",
        "    path_list = []\n",
        "    # reads files from directory, including all subdirectories\n",
        "    for root, _, filenames in os.walk(directory):\n",
        "        for filename in filenames:\n",
        "            if filename.endswith(\".wav\") or filename.endswith(\".mp3\"):\n",
        "                filepath = os.path.join(root, filename)\n",
        "                path_list.append(filepath) # adds file path to list\n",
        "    return path_list # returns a list containing filepaths of all .wav and .mp3 files\n",
        "\n",
        "# creates a dataset of .wav and .mp3 files with a directory\n",
        "def create_dataset_from_directory(directory, label, max_files=None):\n",
        "    \"\"\"Creates a labelled dataset from a directory of .wav and .mp3 files.\"\"\"\n",
        "    path_list = get_file_list(directory)\n",
        "\n",
        "    if max_files is not None:\n",
        "        path_list = path_list[:max_files]\n",
        "\n",
        "    # Create a dataset with the list of file paths\n",
        "    audio_dataset = Dataset.from_dict({\"audio\": path_list})\n",
        "    # Cast the 'audio' column to the Audio feature type\n",
        "    audio_dataset = audio_dataset.cast_column(\"audio\", Audio(sampling_rate=16000, mono=True))\n",
        "\n",
        "    # Convert the dataset to a pandas DataFrame\n",
        "    df = audio_dataset.to_pandas()\n",
        "\n",
        "    # Assign the label to all examples\n",
        "    df['label'] = label\n",
        "\n",
        "    # Define the label names\n",
        "    label_names = ['fake', 'real']\n",
        "\n",
        "    # Define the label feature with the correct ClassLabel\n",
        "    label_feature = ClassLabel(num_classes=len(label_names), names=label_names)\n",
        "\n",
        "    # Define the new features for the dataset\n",
        "    new_features = Features({\n",
        "        'audio': audio_dataset.features['audio'],  # Assuming 'audio' is the audio feature\n",
        "        'label': label_feature\n",
        "    })\n",
        "\n",
        "    # Create a new dataset with the updated features\n",
        "    updated_dataset = Dataset.from_pandas(df, features=new_features)\n",
        "\n",
        "    return updated_dataset"
      ],
      "metadata": {
        "id": "uUXqHM3d9Cid"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create fake and real dataset, pass in the appropriate labels\n",
        "fake_dataset = create_dataset_from_directory(\"/content/KAGGLE/AUDIO/FAKE\", 0)\n",
        "real_dataset = create_dataset_from_directory(\"/content/KAGGLE/AUDIO/REAL\", 1)\n",
        "\n",
        "# Combine the datasets\n",
        "combined_dataset = concatenate_datasets([real_dataset, fake_dataset])\n",
        "\n",
        "# Split the dataset into train and test set\n",
        "combined_dataset = combined_dataset.train_test_split(test_size=0.2)"
      ],
      "metadata": {
        "id": "rsB-BTU-9yAA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The provided code retrieves the labels from a dataset and creates two dictionaries to map labels to their corresponding IDs and vice versa."
      ],
      "metadata": {
        "id": "JOKqfSxk-JwD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the list of labels from the dataset\n",
        "labels = combined_dataset['train'].features[\"label\"].names\n",
        "\n",
        "# Initialize empty dictionaries to store the mappings\n",
        "label2id, id2label = dict(), dict()\n",
        "\n",
        "# Create mappings between labels and their corresponding IDs\n",
        "for i, label in enumerate(labels):\n",
        "    label2id[label] = str(i)  # Map label to ID (as a string)\n",
        "    id2label[str(i)] = label  # Map ID (as a string) to label\n",
        "\n",
        "num_labels = len(id2label)"
      ],
      "metadata": {
        "id": "9wWhOrnY-ELL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following code defines the pre-processor used by the Wav2Vec model"
      ],
      "metadata": {
        "id": "2u3WNkf2-PAd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# loading the processor\n",
        "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base\")\n",
        "\n",
        "# Modify the preprocess_function to use the processor\n",
        "def preprocess_function(examples):\n",
        "    audio_arrays = [x[\"array\"] for x in examples[\"audio\"]]\n",
        "    inputs = processor(\n",
        "        audio_arrays,\n",
        "        sampling_rate=processor.feature_extractor.sampling_rate,\n",
        "        padding=True,\n",
        "        max_length=16000,\n",
        "        truncation=True,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "    inputs[\"labels\"] = examples[\"label\"]\n",
        "    return inputs\n",
        "\n",
        "# applied the pre-process function to the dataset, and remove the audio column.\n",
        "combined_dataset = combined_dataset.map(preprocess_function, remove_columns=\"audio\", batched=True)"
      ],
      "metadata": {
        "id": "X8pWvFWu-T4V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "now the computation metrics are defined"
      ],
      "metadata": {
        "id": "QUJicBIq--UM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = evaluate.load(\"accuracy\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions = np.argmax(eval_pred.predictions, axis=1)\n",
        "    return accuracy.compute(predictions=predictions, references=eval_pred.label_ids)"
      ],
      "metadata": {
        "id": "DSAjssOq_C_T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing the pre-trained base model"
      ],
      "metadata": {
        "id": "umjlV40T_H5W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# importing model from HuggingFace Hub\n",
        "model = AutoModelForAudioClassification.from_pretrained(\n",
        "    \"facebook/wav2vec2-base\", num_labels=num_labels, label2id=label2id, id2label=id2label\n",
        ")"
      ],
      "metadata": {
        "id": "wdqILb8v_KR3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparing the training job"
      ],
      "metadata": {
        "id": "kwT26NyrAYBc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up the training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"trained_model\",  # Directory to save the trained model\n",
        "    evaluation_strategy=\"epoch\",  # Evaluate model at the end of each epoch\n",
        "    save_strategy=\"epoch\",  # Save the model at the end of each epoch\n",
        "    learning_rate=1e-3,  # Learning rate for the optimizer\n",
        "    per_device_train_batch_size=32,  # Batch size per device for training\n",
        "    gradient_accumulation_steps=4,  # Number of steps for gradient accumulation\n",
        "    per_device_eval_batch_size=32,  # Batch size per device for evaluation\n",
        "    num_train_epochs=10,  # Number of training epochs\n",
        "    warmup_ratio=0.1,  # Ratio of warmup steps for the learning rate scheduler\n",
        "    logging_steps=10,  # Log training metrics every specified number of steps\n",
        "    load_best_model_at_end=True,  # Load the best model at the end of training\n",
        "    metric_for_best_model=\"accuracy\",  # Metric to use for tracking the best model\n",
        ")\n",
        "\n",
        "# Initialize the trainer\n",
        "trainer = Trainer(\n",
        "    model=model,  # The model to be trained\n",
        "    args=training_args,  # Training arguments\n",
        "    train_dataset=combined_dataset[\"train\"],  # Training dataset\n",
        "    eval_dataset=combined_dataset[\"test\"],  # Evaluation dataset\n",
        "    tokenizer=processor.feature_extractor,  # Tokenizer for pre-processing\n",
        "    compute_metrics=compute_metrics,  # Function to compute evaluation metrics\n",
        ")"
      ],
      "metadata": {
        "id": "FQb29Opy_XVQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Running the training loop"
      ],
      "metadata": {
        "id": "EZgNYLJ8AcPf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "4Etme_PhAeTt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}